{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib torch_geometric"
      ],
      "metadata": {
        "id": "_YWJ-ZWq-8Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import RGCNConv\n",
        "from torch_geometric.utils import negative_sampling\n",
        "import rdflib\n",
        "from rdflib.namespace import Namespace, RDF\n",
        "from shapely import wkt\n",
        "import glob\n",
        "import random\n",
        "import re\n",
        "import requests\n",
        "from sklearn.metrics import precision_score\n",
        "from rdflib.namespace import NamespaceManager"
      ],
      "metadata": {
        "id": "uy5UoTHx88xs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OWNER = \"bxbalaban\"\n",
        "REPO = \"ai-lab\"\n",
        "BRANCH = \"db0c4603d6b5b78929720437b2bbf93ede6ff6b6\"\n",
        "FOLDER_PATH = \"data\"\n",
        "\n",
        "os.makedirs(\"ttl_files\", exist_ok=True)\n",
        "\n",
        "api_url = f\"https://api.github.com/repos/{OWNER}/{REPO}/contents/{FOLDER_PATH}?ref={BRANCH}\"\n",
        "\n",
        "response = requests.get(api_url)\n",
        "response.raise_for_status()\n",
        "files = response.json()\n",
        "\n",
        "ttl_files = [f for f in files if f[\"name\"].endswith(\".ttl\")]\n",
        "\n",
        "print(f\"Found {len(ttl_files)} TTL files.\")\n",
        "\n",
        "for file_info in ttl_files:\n",
        "    download_url = file_info[\"download_url\"]\n",
        "    filename = file_info[\"name\"]\n",
        "    print(f\"Downloading {filename} ...\")\n",
        "    r = requests.get(download_url)\n",
        "    r.raise_for_status()\n",
        "    with open(os.path.join(\"ttl_files\", filename), \"wb\") as f:\n",
        "        f.write(r.content)\n",
        "\n",
        "print(\"All TTL files downloaded to ./ttl_files/\")"
      ],
      "metadata": {
        "id": "IaAUajPH_NOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOTAI = Namespace(\"http://www.aiLab.org/botAiLab#\")\n",
        "GEO = Namespace(\"http://www.opengis.net/ont/geosparql#\")\n",
        "LOCAL = Namespace(\"http://example.org/building/\")\n",
        "\n",
        "class GeoLinkPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels=64, num_relations=3):\n",
        "        super().__init__()\n",
        "        self.conv1 = RGCNConv(in_channels, hidden_channels, num_relations)\n",
        "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_channels * 2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def encode(self, x, edge_index, edge_type):\n",
        "        x = self.conv1(x, edge_index, edge_type).relu()\n",
        "        x = self.conv2(x, edge_index, edge_type)\n",
        "        return x\n",
        "\n",
        "    def decode(self, z, edge_index):\n",
        "        src, dst = edge_index\n",
        "        z_cat = torch.cat([z[src], z[dst]], dim=1)\n",
        "        return self.classifier(z_cat).view(-1)\n",
        "\n",
        "def parse_ttl_to_pyg(ttl_path):\n",
        "    g = rdflib.Graph()\n",
        "    g.parse(ttl_path, format=\"turtle\")\n",
        "    g.bind(\"botAiLab\", BOTAI)\n",
        "    g.bind(\"geo\", GEO)\n",
        "\n",
        "    nodes = list(set(g.subjects()))\n",
        "    node_idx = {n: i for i, n in enumerate(nodes)}\n",
        "    num_nodes = len(nodes)\n",
        "\n",
        "    features = torch.zeros((num_nodes, 7))  # x, y, z, w, h, d, rot\n",
        "\n",
        "    for s in nodes:\n",
        "        loc = g.value(s, BOTAI.hasLocation)\n",
        "        if loc:\n",
        "            try:\n",
        "                coords = list(map(float, str(loc).split(\",\")))\n",
        "                if len(coords) == 3:\n",
        "                    x, y, z = coords\n",
        "                    wkt_str = f\"POINT Z({x} {y} {z})\"\n",
        "                    g.add((s, GEO.asWKT, rdflib.Literal(wkt_str, datatype=GEO.wktLiteral)))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    for s in nodes:\n",
        "        i = node_idx[s]\n",
        "        loc = g.value(s, GEO.asWKT)\n",
        "        if loc:\n",
        "            pt = wkt.loads(str(loc))\n",
        "            features[i][:3] = torch.tensor([pt.x, pt.y, pt.z if hasattr(pt, 'z') else 0.0])\n",
        "\n",
        "        size = g.value(s, BOTAI.hasSize)\n",
        "        if size:\n",
        "            try:\n",
        "                w, h, d = map(float, str(size).split(\",\"))\n",
        "                features[i][3:6] = torch.tensor([w, h, d])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        rot = g.value(s, BOTAI.hasRotation)\n",
        "        if rot:\n",
        "            try:\n",
        "                features[i][6] = float(str(rot))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    relation_uris = [\n",
        "        BOTAI.adjacentElement,\n",
        "        BOTAI.isAbove,\n",
        "        BOTAI.intersectsElement\n",
        "    ]\n",
        "    relation_to_id = {rel: i for i, rel in enumerate(relation_uris)}\n",
        "\n",
        "    edge_list = []\n",
        "    edge_types = []\n",
        "\n",
        "    for rel in relation_uris:\n",
        "        for s, _, o in g.triples((None, rel, None)):\n",
        "            if s in node_idx and o in node_idx:\n",
        "                edge_list.append([node_idx[s], node_idx[o]])\n",
        "                edge_types.append(relation_to_id[rel])\n",
        "\n",
        "    if not edge_list:\n",
        "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "        edge_type = torch.empty((0,), dtype=torch.long)\n",
        "    else:\n",
        "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "        edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
        "\n",
        "    pos_edges = [\n",
        "        [node_idx[s], node_idx[o]]\n",
        "        for s, o in g.subject_objects(BOTAI.supports)\n",
        "        if s in node_idx and o in node_idx\n",
        "    ]\n",
        "    if not pos_edges:\n",
        "        raise ValueError(f\"No supports links found in {ttl_path}\")\n",
        "    pos_edge_index = torch.tensor(pos_edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=pos_edge_index,\n",
        "        num_nodes=num_nodes,\n",
        "        num_neg_samples=pos_edge_index.size(1)\n",
        "    )\n",
        "\n",
        "    return Data(\n",
        "        x=features,\n",
        "        edge_index=edge_index,\n",
        "        edge_type=edge_type,\n",
        "        pos_edge_index=pos_edge_index,\n",
        "        neg_edge_index=neg_edge_index\n",
        "    )\n",
        "\n",
        "def train(model, data, optimizer):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    z = model.encode(data.x, data.edge_index, data.edge_type)\n",
        "    pos_score = model.decode(z, data.pos_edge_index)\n",
        "    neg_score = model.decode(z, data.neg_edge_index)\n",
        "\n",
        "    pos_loss = F.binary_cross_entropy_with_logits(pos_score, torch.ones_like(pos_score))\n",
        "    neg_loss = F.binary_cross_entropy_with_logits(neg_score, torch.zeros_like(neg_score))\n",
        "    loss = pos_loss + neg_loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "def main(data_folder, epochs=100):\n",
        "    ttl_files = glob.glob(os.path.join(data_folder, \"*.ttl\"))\n",
        "\n",
        "    filtered_files = []\n",
        "    for f in ttl_files:\n",
        "        filename = os.path.basename(f)\n",
        "        match = re.search(r'(\\d+)\\.ttl$', filename)\n",
        "        if match:\n",
        "            num = int(match.group(1))\n",
        "            if 1 <= num <= 50:\n",
        "                filtered_files.append(f)\n",
        "\n",
        "    all_graphs = [parse_ttl_to_pyg(f) for f in filtered_files]\n",
        "\n",
        "    model = GeoLinkPredictor(in_channels=7, num_relations=3)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_loss = 0\n",
        "        for graph in all_graphs:\n",
        "            loss = train(model, graph, optimizer)\n",
        "            total_loss += loss\n",
        "        print(f\"Epoch {epoch} - Loss: {total_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"link_predictor_model.pt\")\n",
        "    print(\"‚úÖ Model saved to link_predictor_model.pt\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(data_folder=\"/content/ttl_files\")"
      ],
      "metadata": {
        "id": "k_WHGMypMIj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_graph_for_inference(ttl_file):\n",
        "    g = rdflib.Graph()\n",
        "    g.parse(ttl_file, format=\"turtle\")\n",
        "    g.bind(\"botAiLab\", BOTAI)\n",
        "    g.bind(\"geo\", GEO)\n",
        "\n",
        "    nodes = list(set(g.subjects()))\n",
        "    node_index = {n: i for i, n in enumerate(nodes)}\n",
        "    index_node = {i: n for n, i in node_index.items()}\n",
        "    num_nodes = len(nodes)\n",
        "\n",
        "    features = torch.zeros((num_nodes, 7))  # x, y, z, w, h, d, rot\n",
        "\n",
        "    for s in nodes:\n",
        "        loc = g.value(s, BOTAI.hasLocation)\n",
        "        if loc:\n",
        "            try:\n",
        "                coords = list(map(float, str(loc).split(\",\")))\n",
        "                if len(coords) == 3:\n",
        "                    x, y, z = coords\n",
        "                    wkt_str = f\"POINT Z({x} {y} {z})\"\n",
        "                    g.add((s, GEO.asWKT, rdflib.Literal(wkt_str, datatype=GEO.wktLiteral)))\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    for s in nodes:\n",
        "        i = node_index[s]\n",
        "        loc = g.value(s, GEO.asWKT)\n",
        "        if loc:\n",
        "            pt = wkt.loads(str(loc))\n",
        "            features[i][:3] = torch.tensor([pt.x, pt.y, pt.z if hasattr(pt, 'z') else 0.0])\n",
        "\n",
        "        size = g.value(s, BOTAI.hasSize)\n",
        "        if size:\n",
        "            try:\n",
        "                w, h, d = map(float, str(size).split(\",\"))\n",
        "                features[i][3:6] = torch.tensor([w, h, d])\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        rot = g.value(s, BOTAI.hasRotation)\n",
        "        if rot:\n",
        "            try:\n",
        "                features[i][6] = float(str(rot))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    relation_uris = [\n",
        "        BOTAI.adjacentElement,\n",
        "        BOTAI.isAbove,\n",
        "        BOTAI.intersectsElement\n",
        "    ]\n",
        "\n",
        "    relation_to_id = {rel: i for i, rel in enumerate(relation_uris)}\n",
        "\n",
        "    edge_list = []\n",
        "    edge_types = []\n",
        "\n",
        "    for rel in relation_uris:\n",
        "        for s, _, o in g.triples((None, rel, None)):\n",
        "            if s in node_index and o in node_index:\n",
        "                edge_list.append([node_index[s], node_index[o]])\n",
        "                edge_types.append(relation_to_id[rel])\n",
        "\n",
        "    if not edge_list:\n",
        "        edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "        edge_type = torch.empty((0,), dtype=torch.long)\n",
        "    else:\n",
        "        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
        "        edge_type = torch.tensor(edge_types, dtype=torch.long)\n",
        "\n",
        "    supports = set()\n",
        "    for s, _, o in g.triples((None, BOTAI.supports, None)):\n",
        "        if s in node_index and o in node_index:\n",
        "            supports.add((node_index[s], node_index[o]))\n",
        "\n",
        "    return features, edge_index, supports, node_index, index_node, edge_type"
      ],
      "metadata": {
        "id": "7veyaKe3M-MZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_missing_links(ttl_file, model_path, top_k=10):\n",
        "    original_graph = rdflib.Graph()\n",
        "    original_graph.parse(ttl_file, format=\"turtle\")\n",
        "\n",
        "    test_graph = rdflib.Graph()\n",
        "    test_graph += original_graph\n",
        "    botAiLab = Namespace(\"http://www.aiLab.org/botAiLab#\")\n",
        "    supports_triples = list(test_graph.triples((None, botAiLab.supports, None)))\n",
        "\n",
        "    for triple in supports_triples:\n",
        "        test_graph.remove(triple)\n",
        "\n",
        "    temp_ttl_path = \"temp_inference.ttl\"\n",
        "    test_graph.serialize(temp_ttl_path, format=\"turtle\")\n",
        "\n",
        "    x, edge_index, _ , node_index, index_node, edge_type = load_graph_for_inference(temp_ttl_path)\n",
        "\n",
        "    model = GeoLinkPredictor(in_channels=7)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cpu\")))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(x, edge_index, edge_type)\n",
        "\n",
        "        num_nodes = x.size(0)\n",
        "        candidate_edges = []\n",
        "        candidate_labels = []\n",
        "        for i in range(num_nodes):\n",
        "            for j in range(num_nodes):\n",
        "                if i != j:\n",
        "                    subj = rdflib.URIRef(index_node[i])\n",
        "                    obj = rdflib.URIRef(index_node[j])\n",
        "                    candidate_edges.append((i, j))\n",
        "                    label = (subj, botAiLab.supports, obj) in original_graph\n",
        "                    candidate_labels.append(label)\n",
        "\n",
        "        if not candidate_edges:\n",
        "            print(\"‚úÖ No candidate missing links to predict.\")\n",
        "            return\n",
        "\n",
        "        edge_tensor = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
        "        scores = torch.sigmoid(model.decode(z, edge_tensor))\n",
        "\n",
        "        top_scores, top_indices = torch.topk(scores, min(top_k, len(scores)))\n",
        "        filtered_scores = []\n",
        "        filtered_indices = []\n",
        "        for i, score in enumerate(top_scores):\n",
        "            if score >= 0.9:\n",
        "                filtered_scores.append(score)\n",
        "                filtered_indices.append(top_indices[i])\n",
        "\n",
        "        predicted_links = [candidate_edges[idx] for idx in filtered_indices]\n",
        "        predicted_labels = [candidate_labels[idx] for idx in filtered_indices]\n",
        "\n",
        "\n",
        "        print(f\"üìä Top {top_k} predicted missing `botAiLab:supports` links:\")\n",
        "        for score, (i, j), is_correct in zip(filtered_scores, predicted_links, predicted_labels):\n",
        "            subj_uri = rdflib.URIRef(index_node[i])\n",
        "            obj_uri = rdflib.URIRef(index_node[j])\n",
        "            status = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
        "            print(f\"{status} {subj_uri} ‚Üí {obj_uri} | score={score:.4f}\")\n",
        "            original_graph.add((subj_uri, botAiLab.supports, obj_uri))\n",
        "\n",
        "        folder = os.path.dirname(ttl_file)\n",
        "        filename = os.path.basename(ttl_file)\n",
        "        enriched_path = os.path.join(folder, f\"enriched_{filename}\")\n",
        "\n",
        "        namespace_manager = NamespaceManager(original_graph)\n",
        "        namespace_manager.bind(\"local\", local_ns)\n",
        "        original_graph.namespace_manager = namespace_manager\n",
        "        original_graph.serialize(destination=enriched_path, format=\"turtle\")\n",
        "\n",
        "        accuracy = sum(predicted_labels) / len(predicted_labels)\n",
        "        print(f\"\\n Accuracy of top-{top_k} predictions: {accuracy:.2%}\")\n",
        "        return accuracy"
      ],
      "metadata": {
        "id": "LYpJYsysYHIy"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_missing_links_in_folder(folder_path, model_path, top_k=50):\n",
        "    ttl_files = glob.glob(os.path.join(folder_path, \"*.ttl\"))\n",
        "\n",
        "    filtered_files = []\n",
        "    for f in ttl_files:\n",
        "        filename = os.path.basename(f)\n",
        "        match = re.search(r'^building(\\d+)\\.ttl$', filename)\n",
        "        if match:\n",
        "            num = int(match.group(1))\n",
        "            if 90 <= num <= 99:\n",
        "                filtered_files.append(f)\n",
        "\n",
        "    if not filtered_files:\n",
        "        print(\"üö´ No TTL files found in the folder.\")\n",
        "        return\n",
        "    all_acc = []\n",
        "    for ttl_file in filtered_files:\n",
        "        full_path = os.path.join(folder_path, ttl_file)\n",
        "        print(f\"\\nüîç Processing: {ttl_file}\")\n",
        "        try:\n",
        "            acc = predict_missing_links(\n",
        "                ttl_file=full_path,\n",
        "                model_path=model_path,\n",
        "                top_k=top_k\n",
        "            )\n",
        "            all_acc.append(acc)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Failed to process {ttl_file}: {e}\")\n",
        "\n",
        "    print(f\"Average Acc: {sum(all_acc)/len(all_acc):.4f}\")"
      ],
      "metadata": {
        "id": "YSH3xy9jYqJC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_missing_links_in_folder(\n",
        "    folder_path=\"/content/ttl_files\",\n",
        "    model_path=\"/content/link_predictor_model.pt\",\n",
        "    top_k=15\n",
        ")"
      ],
      "metadata": {
        "id": "_I7B9FJmX_tb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}